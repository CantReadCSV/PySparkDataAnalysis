# pyspark-basics

This Git repository contains a set of easy examples to get started with Apache Spark using Python. 

## Getting Started

The instructions below give a basic overview over the required dependencies to run the code in each of the sub-folders.

### Prerequisites

I recommend the Anaconda distribution in order to get started with [PySpark](https://anaconda.org/conda-forge/pyspark). To do so, run the following command in the Anaconda shell:

```
conda install -c conda-forge pyspark 
```
___Note:___ The code has been tested with pyspark version 2.4.3, although I belive the code will stay compatible with future minor releases, I strongly recommend using the same version.

### Dataset
As part of the basics, several data files are used, which are not part of the repository. Instructions for download of these files is provided for each of the examples.


## Running code

Each of the examples may be executed independently, the following structure is in place:
 - 00_Basics
 - Folder2
 - Folder3
 - Folder4

### 

To run each of the given examples, simply execute the following command:
```
python <filename.py>
```

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

## Acknowledgments

* [Taming Big Data with Apache Spark and Python - Hands On](https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/)
* [UCSanDiegoX: DSE230x
Big Data Analytics Using Spark](https://www.edx.org/v2/course/big-data-analytics-using-spark-2)
